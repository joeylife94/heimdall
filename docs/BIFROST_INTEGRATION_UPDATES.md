# ğŸŒˆ Bifrost Integration Updates Guide

> **Heimdall ì—°ë™ì„ ìœ„í•œ Bifrost í”„ë¡œì íŠ¸ ì—…ë°ì´íŠ¸ ê°€ì´ë“œ**

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [Kafka í† í”½ ì„¤ì •](#kafka-í† í”½-ì„¤ì •)
3. [ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ êµ¬í˜„](#ì´ë²¤íŠ¸-ìŠ¤í‚¤ë§ˆ-êµ¬í˜„)
4. [Kafka Consumer êµ¬í˜„](#kafka-consumer-êµ¬í˜„)
5. [Kafka Producer êµ¬í˜„](#kafka-producer-êµ¬í˜„)
6. [ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸](#ì„¤ì •-íŒŒì¼-ì—…ë°ì´íŠ¸)
7. [API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€](#api-ì—”ë“œí¬ì¸íŠ¸-ì¶”ê°€)
8. [í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](#í…ŒìŠ¤íŠ¸-ê°€ì´ë“œ)

---

## ê°œìš”

Heimdallê³¼ì˜ í†µí•©ì„ ìœ„í•´ Bifrost í”„ë¡œì íŠ¸ì— ë‹¤ìŒ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤:

### í•„ìˆ˜ êµ¬í˜„ ì‚¬í•­
- âœ… Kafka Consumer: `analysis.request` í† í”½ì—ì„œ ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ 
- âœ… Kafka Producer: `analysis.result` í† í”½ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ ë°œí–‰
- âœ… ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜ (JSON ê¸°ë°˜)
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ë° DLQ(Dead Letter Queue) ì²˜ë¦¬
- âœ… ìƒíƒœ ê´€ë¦¬ ë° ë¡œê¹…

### ì„ íƒì  êµ¬í˜„ ì‚¬í•­
- ğŸ”² REST API: Heimdallì´ ì§ì ‘ ë¶„ì„ ìš”ì²­í•  ìˆ˜ ìˆëŠ” HTTP ì—”ë“œí¬ì¸íŠ¸
- ğŸ”² WebSocket: ì‹¤ì‹œê°„ ë¶„ì„ ìƒíƒœ ì—…ë°ì´íŠ¸
- ğŸ”² gRPC: ê³ ì„±ëŠ¥ ì–‘ë°©í–¥ í†µì‹ 

---

## Kafka í† í”½ ì„¤ì •

### 1. í•„ìš”í•œ í† í”½ ëª©ë¡

```bash
# Kafka í† í”½ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
kafka-topics.sh --create --bootstrap-server localhost:9092 \
  --topic analysis.request \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=604800000

kafka-topics.sh --create --bootstrap-server localhost:9092 \
  --topic analysis.result \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=604800000

kafka-topics.sh --create --bootstrap-server localhost:9092 \
  --topic dlq.failed \
  --partitions 1 \
  --replication-factor 2 \
  --config retention.ms=2592000000
```

### 2. í† í”½ ì„¤ì • ê¶Œì¥ì‚¬í•­

| í† í”½ | Partitions | Replication | Retention |
|------|------------|-------------|-----------|
| `analysis.request` | 3 | 2 | 7 days |
| `analysis.result` | 3 | 2 | 7 days |
| `dlq.failed` | 1 | 2 | 30 days |

---

## ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ êµ¬í˜„

### 1. ë¶„ì„ ìš”ì²­ ì´ë²¤íŠ¸ (Bifrostê°€ ìˆ˜ì‹ )

**Topic**: `analysis.request`

**Python ëª¨ë¸**:
```python
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime
from enum import Enum

class AnalysisPriority(str, Enum):
    LOW = "LOW"
    NORMAL = "NORMAL"
    HIGH = "HIGH"
    CRITICAL = "CRITICAL"

class AnalysisRequestEvent(BaseModel):
    """Heimdallë¡œë¶€í„° ë°›ëŠ” ë¶„ì„ ìš”ì²­ ì´ë²¤íŠ¸"""
    
    request_id: str = Field(..., description="ìš”ì²­ ê³ ìœ  ID (UUID)")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    log_id: int = Field(..., description="Heimdallì˜ ë¡œê·¸ ID")
    log_content: str = Field(..., description="ë¶„ì„í•  ë¡œê·¸ ë‚´ìš©")
    service_name: str = Field(..., description="ë¡œê·¸ ì¶œì²˜ ì„œë¹„ìŠ¤ëª…")
    environment: str = Field(..., description="í™˜ê²½ (dev/staging/prod)")
    analysis_type: str = Field(default="error", description="ë¶„ì„ ìœ í˜•")
    priority: AnalysisPriority = Field(default=AnalysisPriority.NORMAL)
    callback_topic: str = Field(default="analysis.result")
    correlation_id: str = Field(..., description="ì¶”ì ìš© Correlation ID")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
    
    class Config:
        json_schema_extra = {
            "example": {
                "request_id": "550e8400-e29b-41d4-a716-446655440000",
                "timestamp": "2024-11-12T10:30:00Z",
                "log_id": 12345,
                "log_content": "ERROR: Connection timeout to database",
                "service_name": "user-service",
                "environment": "production",
                "analysis_type": "error",
                "priority": "HIGH",
                "callback_topic": "analysis.result",
                "correlation_id": "corr-123456"
            }
        }
```

### 2. ë¶„ì„ ê²°ê³¼ ì´ë²¤íŠ¸ (Bifrostê°€ ë°œí–‰)

**Topic**: `analysis.result`

**Python ëª¨ë¸**:
```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime
from decimal import Decimal

class AnalysisResultData(BaseModel):
    """AI ë¶„ì„ ê²°ê³¼ ë°ì´í„°"""
    summary: str = Field(..., description="ë¶„ì„ ìš”ì•½")
    root_cause: str = Field(..., description="ê·¼ë³¸ ì›ì¸")
    recommendation: str = Field(..., description="í•´ê²° ê¶Œì¥ì‚¬í•­")
    severity: str = Field(..., description="ì‹¬ê°ë„ (LOW/MEDIUM/HIGH/CRITICAL)")
    confidence: Decimal = Field(..., ge=0, le=1, description="ì‹ ë¢°ë„ (0~1)")

class AnalysisResultEvent(BaseModel):
    """Heimdallë¡œ ë³´ë‚´ëŠ” ë¶„ì„ ê²°ê³¼ ì´ë²¤íŠ¸"""
    
    request_id: str = Field(..., description="ì›ë³¸ ìš”ì²­ ID")
    correlation_id: str = Field(..., description="Correlation ID")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    log_id: int = Field(..., description="Heimdallì˜ ë¡œê·¸ ID")
    analysis_result: AnalysisResultData
    bifrost_analysis_id: int = Field(..., description="Bifrost ë¶„ì„ ID")
    model: str = Field(..., description="ì‚¬ìš©ëœ AI ëª¨ë¸")
    duration_seconds: Decimal = Field(..., description="ë¶„ì„ ì†Œìš” ì‹œê°„")
    
    class Config:
        json_schema_extra = {
            "example": {
                "request_id": "550e8400-e29b-41d4-a716-446655440000",
                "correlation_id": "corr-123456",
                "timestamp": "2024-11-12T10:30:15Z",
                "log_id": 12345,
                "analysis_result": {
                    "summary": "PostgreSQL ì—°ê²° íƒ€ì„ì•„ì›ƒ ë°œìƒ",
                    "root_cause": "Connection pool ê³ ê°ˆë¡œ ì¸í•œ ì—°ê²° ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼",
                    "recommendation": "max_connections ì„¤ì • ì¦ê°€ ë° connection pool í¬ê¸° ì¡°ì • ê¶Œì¥",
                    "severity": "HIGH",
                    "confidence": 0.95
                },
                "bifrost_analysis_id": 789,
                "model": "mistral-7b",
                "duration_seconds": 2.5
            }
        }
```

---

## Kafka Consumer êµ¬í˜„

### Python (aiokafka ì‚¬ìš©)

```python
import asyncio
import json
from aiokafka import AIOKafkaConsumer
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class AnalysisRequestConsumer:
    """ë¶„ì„ ìš”ì²­ì„ ì†Œë¹„í•˜ëŠ” Kafka Consumer"""
    
    def __init__(
        self,
        bootstrap_servers: str = "localhost:9092",
        group_id: str = "bifrost-consumer-group",
        topics: list = None
    ):
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id
        self.topics = topics or ["analysis.request"]
        self.consumer: Optional[AIOKafkaConsumer] = None
        
    async def start(self):
        """Consumer ì‹œì‘"""
        self.consumer = AIOKafkaConsumer(
            *self.topics,
            bootstrap_servers=self.bootstrap_servers,
            group_id=self.group_id,
            auto_offset_reset='earliest',
            enable_auto_commit=False,  # Manual commit
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        await self.consumer.start()
        logger.info(f"Kafka consumer started for topics: {self.topics}")
        
    async def stop(self):
        """Consumer ì¢…ë£Œ"""
        if self.consumer:
            await self.consumer.stop()
            logger.info("Kafka consumer stopped")
    
    async def consume_messages(self, processor_func):
        """
        ë©”ì‹œì§€ ì†Œë¹„ ë° ì²˜ë¦¬
        
        Args:
            processor_func: ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜ (async)
        """
        try:
            async for message in self.consumer:
                try:
                    # ì´ë²¤íŠ¸ íŒŒì‹±
                    event = AnalysisRequestEvent(**message.value)
                    logger.info(f"Received analysis request: {event.request_id}")
                    
                    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
                    await processor_func(event)
                    
                    # Manual commit
                    await self.consumer.commit()
                    
                except Exception as e:
                    logger.error(f"Error processing message: {e}", exc_info=True)
                    # DLQë¡œ ì „ì†¡ (êµ¬í˜„ í•„ìš”)
                    await self._send_to_dlq(message)
                    
        except Exception as e:
            logger.error(f"Consumer error: {e}", exc_info=True)
            raise
    
    async def _send_to_dlq(self, message):
        """ì‹¤íŒ¨í•œ ë©”ì‹œì§€ë¥¼ DLQë¡œ ì „ì†¡"""
        # DLQ Producer êµ¬í˜„ í•„ìš”
        pass

# ì‚¬ìš© ì˜ˆì‹œ
async def process_analysis_request(event: AnalysisRequestEvent):
    """ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ë¡œì§"""
    logger.info(f"Processing log_id: {event.log_id}")
    
    # 1. AI ëª¨ë¸ë¡œ ë¡œê·¸ ë¶„ì„
    analysis_result = await analyze_log_with_ai(event.log_content)
    
    # 2. ê²°ê³¼ë¥¼ Kafkaë¡œ ë°œí–‰
    await send_analysis_result(event, analysis_result)

async def main():
    consumer = AnalysisRequestConsumer()
    await consumer.start()
    try:
        await consumer.consume_messages(process_analysis_request)
    finally:
        await consumer.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

---

## Kafka Producer êµ¬í˜„

### Python (aiokafka ì‚¬ìš©)

```python
import json
from aiokafka import AIOKafkaProducer
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class AnalysisResultProducer:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°œí–‰í•˜ëŠ” Kafka Producer"""
    
    def __init__(self, bootstrap_servers: str = "localhost:9092"):
        self.bootstrap_servers = bootstrap_servers
        self.producer: Optional[AIOKafkaProducer] = None
        
    async def start(self):
        """Producer ì‹œì‘"""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8'),
            acks='all',  # ëª¨ë“  replica í™•ì¸
            retries=3,
            max_in_flight_requests_per_connection=1  # Ordering ë³´ì¥
        )
        await self.producer.start()
        logger.info("Kafka producer started")
        
    async def stop(self):
        """Producer ì¢…ë£Œ"""
        if self.producer:
            await self.producer.stop()
            logger.info("Kafka producer stopped")
    
    async def send_analysis_result(
        self,
        result_event: AnalysisResultEvent,
        topic: str = "analysis.result"
    ) -> bool:
        """
        ë¶„ì„ ê²°ê³¼ ë°œí–‰
        
        Args:
            result_event: ë¶„ì„ ê²°ê³¼ ì´ë²¤íŠ¸
            topic: ë°œí–‰í•  í† í”½
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
            message = result_event.model_dump()
            
            # KeyëŠ” log_idë¡œ ì„¤ì • (íŒŒí‹°ì…”ë‹)
            key = str(result_event.log_id).encode('utf-8')
            
            # ë©”ì‹œì§€ ë°œí–‰
            await self.producer.send_and_wait(topic, value=message, key=key)
            
            logger.info(
                f"Sent analysis result: request_id={result_event.request_id}, "
                f"log_id={result_event.log_id}"
            )
            return True
            
        except Exception as e:
            logger.error(f"Failed to send analysis result: {e}", exc_info=True)
            return False

# ì‚¬ìš© ì˜ˆì‹œ
async def send_analysis_result(
    request_event: AnalysisRequestEvent,
    analysis_data: AnalysisResultData
):
    """ë¶„ì„ ê²°ê³¼ ì „ì†¡"""
    producer = AnalysisResultProducer()
    await producer.start()
    
    try:
        result_event = AnalysisResultEvent(
            request_id=request_event.request_id,
            correlation_id=request_event.correlation_id,
            log_id=request_event.log_id,
            analysis_result=analysis_data,
            bifrost_analysis_id=123456,  # DBì—ì„œ ìƒì„±ëœ ID
            model="mistral-7b",
            duration_seconds=2.5
        )
        
        await producer.send_analysis_result(result_event)
        
    finally:
        await producer.stop()
```

---

## ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸

### config.yaml (Bifrost ì„¤ì •)

```yaml
# Kafka ì„¤ì •
kafka:
  bootstrap_servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
  consumer:
    group_id: bifrost-consumer-group
    auto_offset_reset: earliest
    enable_auto_commit: false
    max_poll_records: 100
    session_timeout_ms: 30000
  producer:
    acks: all
    retries: 3
    max_in_flight_requests_per_connection: 1
    compression_type: snappy
  topics:
    analysis_request: analysis.request
    analysis_result: analysis.result
    dlq: dlq.failed

# Heimdall ì—°ë™ ì„¤ì •
heimdall:
  enabled: true
  callback_topic: analysis.result
  timeout_seconds: 60
  retry_attempts: 3
  retry_backoff_seconds: 5

# AI ëª¨ë¸ ì„¤ì •
ai:
  model: mistral-7b
  max_tokens: 2048
  temperature: 0.7
  timeout: 30
```

### .env íŒŒì¼

```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# Database
DATABASE_URL=postgresql://bifrost:password@postgres:5432/bifrost

# AI Model
AI_MODEL=mistral-7b
AI_API_KEY=your-api-key-here

# Logging
LOG_LEVEL=INFO
```

---

## API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

### FastAPI ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒì )

```python
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/analysis", tags=["analysis"])

class AnalysisRequest(BaseModel):
    log_content: str
    service_name: str
    environment: str
    priority: str = "NORMAL"

class AnalysisResponse(BaseModel):
    analysis_id: int
    status: str
    message: str

@router.post("/request", response_model=AnalysisResponse)
async def request_analysis(
    request: AnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    Heimdallì´ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ë¶„ì„ ìš”ì²­ API
    (Kafka ëŒ€ì‹  HTTPë¡œ ìš”ì²­)
    """
    try:
        # ë¶„ì„ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
        analysis_id = await create_analysis_job(request)
        background_tasks.add_task(process_analysis, analysis_id)
        
        return AnalysisResponse(
            analysis_id=analysis_id,
            status="ACCEPTED",
            message="Analysis request accepted"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/result/{analysis_id}")
async def get_analysis_result(analysis_id: int):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    result = await fetch_analysis_result(analysis_id)
    if not result:
        raise HTTPException(status_code=404, detail="Analysis not found")
    return result
```

---

## í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

### 1. Kafka ì—°ë™ í…ŒìŠ¤íŠ¸

```python
import pytest
import asyncio
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_consume_analysis_request():
    """ë¶„ì„ ìš”ì²­ ì†Œë¹„ í…ŒìŠ¤íŠ¸"""
    consumer = AnalysisRequestConsumer()
    
    # Mock processor
    mock_processor = AsyncMock()
    
    # Mock ë©”ì‹œì§€
    mock_message = {
        "request_id": "test-123",
        "log_id": 12345,
        "log_content": "ERROR: Test error",
        "service_name": "test-service",
        "environment": "test",
        "analysis_type": "error",
        "priority": "NORMAL",
        "callback_topic": "analysis.result",
        "correlation_id": "corr-123"
    }
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    with patch.object(consumer, 'consumer') as mock_consumer:
        mock_consumer.__aiter__.return_value = [mock_message]
        await consumer.consume_messages(mock_processor)
        
    # ê²€ì¦
    mock_processor.assert_called_once()

@pytest.mark.asyncio
async def test_send_analysis_result():
    """ë¶„ì„ ê²°ê³¼ ë°œí–‰ í…ŒìŠ¤íŠ¸"""
    producer = AnalysisResultProducer()
    await producer.start()
    
    result_event = AnalysisResultEvent(
        request_id="test-123",
        correlation_id="corr-123",
        log_id=12345,
        analysis_result=AnalysisResultData(
            summary="Test summary",
            root_cause="Test cause",
            recommendation="Test recommendation",
            severity="LOW",
            confidence=0.9
        ),
        bifrost_analysis_id=789,
        model="test-model",
        duration_seconds=1.0
    )
    
    success = await producer.send_analysis_result(result_event)
    assert success is True
    
    await producer.stop()
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸

```bash
# Docker Composeë¡œ Kafka í™˜ê²½ ì‹¤í–‰
docker-compose -f docker-compose.test.yml up -d

# Python í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/integration/test_heimdall_integration.py -v

# Kafka ë©”ì‹œì§€ í™•ì¸
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic analysis.result --from-beginning
```

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì™„ë£Œ í™•ì¸

- [ ] Kafka Consumer êµ¬í˜„ (`analysis.request` êµ¬ë…)
- [ ] Kafka Producer êµ¬í˜„ (`analysis.result` ë°œí–‰)
- [ ] ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜ (Pydantic ëª¨ë¸)
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° DLQ ì²˜ë¦¬
- [ ] ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì¶”ê°€
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

### ë°°í¬ ì „ í™•ì¸

- [ ] Kafka í† í”½ ìƒì„± ì™„ë£Œ
- [ ] Heimdallê³¼ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
- [ ] ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ë¶€í•˜ í…ŒìŠ¤íŠ¸)
- [ ] ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (Kafka ë‹¤ìš´, ë„¤íŠ¸ì›Œí¬ ëŠê¹€)
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì„¤ì •
- [ ] ì•Œë¦¼ ì„¤ì • (Consumer Lag, ì—ëŸ¬ìœ¨)

---

## ì¶”ê°€ ì°¸ê³  ìë£Œ

- [Kafka Python Client (aiokafka)](https://aiokafka.readthedocs.io/)
- [Pydantic ë¬¸ì„œ](https://docs.pydantic.dev/)
- [FastAPI ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…](https://fastapi.tiangolo.com/tutorial/background-tasks/)
- [Kafka ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](https://kafka.apache.org/documentation/#bestpractices)

---

**ë¬¸ì„œ ë²„ì „**: 1.0.0  
**ìµœì¢… ìˆ˜ì •**: 2024-11-12  
**ì‘ì„±ì**: Heimdall Development Team  
**ë¬¸ì˜**: í†µí•© ê´€ë ¨ ë¬¸ì˜ëŠ” GitHub Issuesë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.
